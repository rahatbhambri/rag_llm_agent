{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, langchain, os\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.llms import OpenAI \n",
    "from dotenv import load_dotenv \n",
    "from sentence_transformers import SentenceTransformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='paraphrase-MiniLM-L3-v2')\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_pdf(path): \n",
    "    file_loader = PyPDFDirectoryLoader(path)\n",
    "    documents = file_loader.load() \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_spliter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc = text_spliter.split_documents(docs)\n",
    "    print(type(doc), type(doc[0]), len(doc))\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'langchain_core.documents.base.Document'> 253\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "data = read_pdf('./static')\n",
    "doc_list = chunk_data(docs=data)\n",
    "\n",
    "vector = embeddings.embed_query(\"any text\")\n",
    "dimension = len(vector)\n",
    "print(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"lcvector\"\n",
    "\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "        doc_list,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_query(query, k=2):\n",
    "    matching_results = vectorstore_from_docs.similarity_search(query, k=k\n",
    "                                                               )\n",
    "    return matching_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize the local LLM via Ollama\n",
    "llm = Ollama(model=\"mistral\", temperature=0.5)\n",
    "\n",
    "# Load QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def retreive_answers(query):\n",
    "    doc_search=retrieve_query(query)\n",
    "    response= chain.run(input_documents=doc_search, question=query) \n",
    "    return response \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rahat Bhambri possesses a wide range of technical skills including proficiency in programming languages such as Python, Golang, Java, JavaScript, and frameworks like Django, Flask, Vue.js, Spring Boot, and Electron.js. He is also familiar with databases like MySQL, PostgreSQL, MongoDB, and Redis. In terms of DevOps and Cloud technologies, Rahat has experience with AWS (EC2, EMR), Kafka, Docker, Kubernetes, GitLab CI/CD, and Celery. For data science and AI, he is skilled in Natural Language Processing (NLP) using SpaCy and BERT, as well as working with libraries such as Pandas, Seaborn, Matplotlib. Additionally, Rahat has experience with various other tools like PyQt5, Flask-SocketIO, Xlwings, and Tableau.\n"
     ]
    }
   ],
   "source": [
    "our_query= \"What skills do Rahat possess?\"\n",
    "answer=retreive_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
